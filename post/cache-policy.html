<!DOCTYPE html><html lang="zh-TW"><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><script>!function(){try {var d=document.documentElement.classList;d.remove('light','dark');var e=localStorage.getItem('theme');if("system"===e||(!e&&true)){var t="(prefers-color-scheme: dark)",m=window.matchMedia(t);m.media!==t||m.matches?d.add('dark'):d.add('light')}else if(e) d.add(e)}catch(e){}}()</script><title>An introduction to Window TinyLFU | s8508235&#x27;s blog</title><meta name="description" content="TinyLFU is a frequency-based cache admission policy"/><meta property="og:type" content="website"/><meta name="og:title" property="og:title" content="An introduction to Window TinyLFU"/><meta name="og:description" property="og:description" content="TinyLFU is a frequency-based cache admission policy"/><link rel="icon" type="image/png" href="/blog/favicon.ico"/><link rel="apple-touch-icon" href="/blog/favicon.ico"/><meta name="next-head-count" content="10"/><link rel="preload" href="/blog/_next/static/css/e1a287d777908626.css" as="style"/><link rel="stylesheet" href="/blog/_next/static/css/e1a287d777908626.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/blog/_next/static/chunks/polyfills-5cd94c89d3acac5f.js"></script><script src="/blog/_next/static/chunks/webpack-c6e85cc450395c4f.js" defer=""></script><script src="/blog/_next/static/chunks/framework-24fda4117f092221.js" defer=""></script><script src="/blog/_next/static/chunks/main-9f53f1039b2c3c72.js" defer=""></script><script src="/blog/_next/static/chunks/pages/_app-492e99d0a17aabe9.js" defer=""></script><script src="/blog/_next/static/chunks/827-22efc97fb31500a7.js" defer=""></script><script src="/blog/_next/static/chunks/716-a52d3c0d45b6ee73.js" defer=""></script><script src="/blog/_next/static/chunks/552-14dffc3b36c9d95b.js" defer=""></script><script src="/blog/_next/static/chunks/pages/post/%5Bslug%5D-31dba5978a8317b4.js" defer=""></script><script src="/blog/_next/static/Z0_U_cDDsSEVLkTYwbyDH/_buildManifest.js" defer=""></script><script src="/blog/_next/static/Z0_U_cDDsSEVLkTYwbyDH/_ssgManifest.js" defer=""></script><script src="/blog/_next/static/Z0_U_cDDsSEVLkTYwbyDH/_middlewareManifest.js" defer=""></script></head><body><div id="__next" data-reactroot=""><div class="w-full min-h-screen dark:bg-gray-700 dark:text-white"><div class="max-w-screen-sm px-4 py-12 mx-auto antialiased font-body"><header class="flex items-center justify-between  mb-2"><div class="max-w-md"><h1><a class="text-2xl font-black text-blue-800 no-underline font-display dark:text-blue-400" href="/blog">Minimal blog</a></h1></div></header><main><article><header class="mb-8"><h1 class="mb-2 text-5xl font-black leading-none font-display">An introduction to Window TinyLFU</h1><p class="text-sm">April 27, 2021</p></header><div class="mb-4 prose lg:prose-lg dark:prose-dark"><h2>Foreword</h2><p>I will just easily go through techniques used by Window TinyLFU.</p><h2>Why Cache?</h2><p>Because we want to access data fast. The buffering provided by a cache benefits both latency and throughput.</p><h2>Eviction Policies</h2><ul><li>Since we cannot cache all data we have, we need to discard some cache when out of space</li><li>We should consider what the cost is if we choose the policy</li><li>Eviction decisions should be done in an efficient manner, in order to avoid a situation in which the computation and space overheads required to answer these questions surpasses the benefit of using the cache</li><li>Hit rate, latency &amp; space matter</li></ul><h4>Examples</h4><ul><li>Least recently used (LRU): Discards the least recently used items first</li><li>Least frequently used (LFU): Discards data that was least frequently used</li><li>Segment LRU (SLRU):<ul><li>SLRU cache is divided into two segments, a probationary segment and a
protected segment</li><li>Lines in each segment are ordered from the most to the least recently
accessed</li><li>Data from misses is added to the cache at the most recently accessed end of
the probationary segment</li><li>Hits are removed from wherever they currently reside and added to the most
recently accessed end of the protected segment</li><li>Lines in the protected segment have thus been accessed at least twice</li></ul></li></ul><h2>Probability data structure</h2><h4>Examples</h4><ul><li>Bloom Filter<ul><li>Pairwise independent hash functions</li><li>Used to test whether an element is a member of a set</li><li>Two result: &quot;possibly in set&quot; or &quot;definitely not in set&quot;</li><li>isInSet(input) = (h1(input) and h2(input) and h3(input) ...)</li></ul></li><li>Count-min Sketch<ul><li>Pairwise independent hash functions</li><li>Used to count elements</li><li>count(input) = min(h1(input), h2(input), h3(input), ...)</li></ul></li></ul><h2>Admission policy</h2><ul><li>Hope to boost the effectiveness of caches</li><li>Given a newly accessed item and an eviction candidate from the cache, our
scheme decides, based on the recent access history, whether it is worth
admitting the new item into the cache at the expense of the eviction candidate</li><li>An accessed item is only inserted into the cache if an admission policy
decides that the cache hit ratio is likely to benefit from replacing it with the
cache victim</li></ul><h3>TinyLFU</h3><p>TinyLFU admission policy use Bloom Filter and Count-min Sketch to trace state of elements.
<span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27513%27%20height=%27183%27/%3e"/></span><img alt="TinyLFU structure" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="w-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;filter:blur(20px);background-size:cover;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAIAAAAhqtkfAAAAVklEQVR42gFLALT/AP////z8/fT08/j5+Zyv0pSpz/n6+////wD6+vr4+Pjy8/Xj5u+Vqs+Qpszi5e/z9PgA9PT19fTz7e/1w8vg0tjl2d/pytLl2t/sgqFAJ1oP4mAAAAAASUVORK5CYII=&quot;);background-position:0% 0%"/><noscript><img alt="TinyLFU structure" srcSet="https://s8508235.github.io/blog/blog/_next/static/media/tinylfu.78903649.png?auto=format&amp;fit=max&amp;w=640 1x, https://s8508235.github.io/blog/blog/_next/static/media/tinylfu.78903649.png?auto=format&amp;fit=max&amp;w=1080 2x" src="https://s8508235.github.io/blog/blog/_next/static/media/tinylfu.78903649.png?auto=format&amp;fit=max&amp;w=1080" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="w-full" loading="lazy"/></noscript></span></p><p>If there is a new element insert event that makes the main cache(SLRU) discard its element, they will be compared with the
state of TinyLFU admission policy (Bloom Filter and Count-min Sketch).
<span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27328%27%20height=%27179%27/%3e"/></span><img alt="TinyLFU with Main Cache" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="w-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;filter:blur(20px);background-size:cover;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAECAIAAAA8r+mnAAAATElEQVR42h2IWw4AIQgDvf9V9UPeoBJWt2mazjQWQRZAQmIWmwB776pqEcFq8Kyqv3/3nGz1R+yxmPc++hii1jJzrcXESDQB4RbJPT4JxFw9HqhgCwAAAABJRU5ErkJggg==&quot;);background-position:0% 0%"/><noscript><img alt="TinyLFU with Main Cache" srcSet="https://s8508235.github.io/blog/blog/_next/static/media/tinylfu-2.584a5f83.png?auto=format&amp;fit=max&amp;w=384 1x, https://s8508235.github.io/blog/blog/_next/static/media/tinylfu-2.584a5f83.png?auto=format&amp;fit=max&amp;w=750 2x" src="https://s8508235.github.io/blog/blog/_next/static/media/tinylfu-2.584a5f83.png?auto=format&amp;fit=max&amp;w=750" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="w-full" loading="lazy"/></noscript></span></p><p>Finally, for sparse bursts, the paper suggest a LRU as window cache.
<span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27466%27%20height=%27184%27/%3e"/></span><img alt="W-TinyLFU" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="w-full" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;filter:blur(20px);background-size:cover;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAADCAIAAAAhqtkfAAAAQ0lEQVR42g2KWwrAMAgEvf9JWwop9Rm1YvzYZWAG/iok/pCQZJ5FZ5EJavu6H1HzSFYbsdZLrFBVk3T3gG13D2bxyAMcQUVA39rppgAAAABJRU5ErkJggg==&quot;);background-position:0% 0%"/><noscript><img alt="W-TinyLFU" srcSet="https://s8508235.github.io/blog/blog/_next/static/media/w-tinylfu.57ab4fa5.png?auto=format&amp;fit=max&amp;w=640 1x, https://s8508235.github.io/blog/blog/_next/static/media/w-tinylfu.57ab4fa5.png?auto=format&amp;fit=max&amp;w=1080 2x" src="https://s8508235.github.io/blog/blog/_next/static/media/w-tinylfu.57ab4fa5.png?auto=format&amp;fit=max&amp;w=1080" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="w-full" loading="lazy"/></noscript></span></p><h2>Reference</h2><ul><li><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Cache_(computing)">Cache(wikipedia)</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Cache_replacement_policies">Cache replacement policies</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://github.com/ben-manes/caffeine/wiki/Efficiency">Caffeine wiki</a></li><li><a target="_blank" rel="noopener noreferrer" href="https://dl.acm.org/doi/10.1145/3149371">Window TinyLFU paper</a></li></ul></div><hr class="mt-4"/><footer><div class="flex items-center space-x-3 mt-8 mb-16"><div class="flex-shrink-0 mb-0 overflow-hidden rounded-full w-14 h-14"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2756%27%20height=%2756%27/%3e"/></span><img alt="Profile" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;filter:blur(20px);background-size:cover;background-image:url(&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAHCAAAAAAQMlOCAAAASklEQVR42gE/AMD/AG/Q1dbUplpPAKD/7eHs1QBBAKD85tivzKBnAIbs7eDW3/ClAIvJyODi3ue9ACmwueTk4ejCACBf19TX1N6z530ojUkGi+cAAAAASUVORK5CYII=&quot;);background-position:0% 0%"/><noscript><img alt="Profile" srcSet="https://s8508235.github.io/blog/blog/_next/static/media/profile.7ee99ab4.png?auto=format&amp;fit=max&amp;w=64 1x, https://s8508235.github.io/blog/blog/_next/static/media/profile.7ee99ab4.png?auto=format&amp;fit=max&amp;w=128 2x" src="https://s8508235.github.io/blog/blog/_next/static/media/profile.7ee99ab4.png?auto=format&amp;fit=max&amp;w=128" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span></div><p class="text-base leading-7">Written by<!-- --> <a class="font-semibold" target="_blank" rel="noreferrer noopener" href="https://github.com/s8508235">s8508235</a> <b>[ZH-TW/EN]</b><br/>Before being good at writing, I have to start writing.<!-- --> </p></div></footer></article><nav class="flex flex-wrap justify-between mb-10"><a class="text-lg font-bold" href="/blog/post/2021-march-note">← <!-- -->2021 March note</a><a class="text-lg font-bold" href="/blog/post/openssl-escape-character">Openssl escape character<!-- --> →</a></nav></main><footer class="text-lg font-light">© <!-- -->2022<!-- --> If you have any questions, please send me <a href="mailto:a8508235@gmail.com">an email</a>!</footer></div></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"frontmatter":{"title":"An introduction to Window TinyLFU","date":"April 27, 2021","description":"TinyLFU is a frequency-based cache admission policy"},"post":{"content":" \n## Foreword\nI will just easily go through techniques used by Window TinyLFU.\n\n## Why Cache?\nBecause we want to access data fast. The buffering provided by a cache benefits both latency and throughput.\n\n## Eviction Policies\n- Since we cannot cache all data we have, we need to discard some cache when out of space\n- We should consider what the cost is if we choose the policy\n- Eviction decisions should be done in an efficient manner, in order to avoid a situation in which the computation and space overheads required to answer these questions surpasses the benefit of using the cache\n- Hit rate, latency \u0026 space matter\n\n#### Examples\n- Least recently used (LRU): Discards the least recently used items first\n- Least frequently used (LFU): Discards data that was least frequently used\n- Segment LRU (SLRU):\n  - SLRU cache is divided into two segments, a probationary segment and a\nprotected segment\n  - Lines in each segment are ordered from the most to the least recently\naccessed\n  - Data from misses is added to the cache at the most recently accessed end of\nthe probationary segment\n  - Hits are removed from wherever they currently reside and added to the most\nrecently accessed end of the protected segment\n  - Lines in the protected segment have thus been accessed at least twice\n\n## Probability data structure\n\n#### Examples\n- Bloom Filter\n  - Pairwise independent hash functions\n  - Used to test whether an element is a member of a set\n  - Two result: \"possibly in set\" or \"definitely not in set\"\n  - isInSet(input) = (h1(input) and h2(input) and h3(input) ...)\n- Count-min Sketch\n  - Pairwise independent hash functions\n  - Used to count elements\n  - count(input) = min(h1(input), h2(input), h3(input), ...)\n\n## Admission policy\n- Hope to boost the effectiveness of caches\n- Given a newly accessed item and an eviction candidate from the cache, our\nscheme decides, based on the recent access history, whether it is worth\nadmitting the new item into the cache at the expense of the eviction candidate\n- An accessed item is only inserted into the cache if an admission policy\ndecides that the cache hit ratio is likely to benefit from replacing it with the\ncache victim\n\n### TinyLFU\nTinyLFU admission policy use Bloom Filter and Count-min Sketch to trace state of elements.\n![TinyLFU structure](cache-policy-pics/tinylfu.png)\n\nIf there is a new element insert event that makes the main cache(SLRU) discard its element, they will be compared with the\nstate of TinyLFU admission policy (Bloom Filter and Count-min Sketch).\n![TinyLFU with Main Cache](cache-policy-pics/tinylfu-2.png)\n\nFinally, for sparse bursts, the paper suggest a LRU as window cache.\n![W-TinyLFU](cache-policy-pics/w-tinylfu.png)\n\n## Reference\n- [Cache(wikipedia)](https://en.wikipedia.org/wiki/Cache_(computing))\n- [Cache replacement policies](https://en.wikipedia.org/wiki/Cache_replacement_policies)\n- [Caffeine wiki](https://github.com/ben-manes/caffeine/wiki/Efficiency)\n- [Window TinyLFU paper](https://dl.acm.org/doi/10.1145/3149371)","excerpt":""},"previousPost":{"slug":"2021-march-note","frontmatter":{"title":"2021 March note","date":"March 27, 2021","description":"webrtc/linux/network reading"},"excerpt":"","content":" \n## 聲明\n我想筆記摘要是作給未來的自己用的，不過如果剛好有人看到我的筆記去讀原文的話，那是再好不過。\u003cdel\u003e有人只看筆記不看課本的嗎\u003c/del\u003e\n\n如果有錯也歡迎糾正，謝謝指教。\n\n## Reading\n- [Why video chat is a hard technical problem](https://dev.to/lazerwalker/why-video-chat-is-a-hard-technical-problem-43gj)\n  - 本文主要是開發 webrtc 會遇到的問題\n  - 瀏覽器支援不一\n  - 需要自己的後端並且可能需要面對過多 peer to peer 連線 ( Websocket -\u003e STUN -\u003e SFU)\n  - 應該也是因為這樣才會有第三方 SDK 在做這些事情吧\n- [TLS 1.3 / QUIC 與 HTTP/3 對效能的改善](https://hkt999.medium.com/tls-1-3-quic-%E8%88%87-http-3-%E5%B0%8D%E6%95%88%E8%83%BD%E7%9A%84%E6%94%B9%E5%96%84-a37b2ddcfc95)\n  - 本文主要是 QUIC 介紹\n  - [latency](https://www.igvita.com/2012/07/19/latency-the-new-web-performance-bottleneck) 影響\n  - 基於 UDP\n  - TLS 1.3 的 handshake 會先 Client HELLO (CHLO) 來節省 RTT\n  - QUIC 的 handshake 類似 TLS 1.3 ( inchoate CHLO 似乎有可能一次傳不完)\n  - 解決 TCP retransmission ambiguity problem\n  - 這個[pdf](https://www.uio.no/studier/emner/matnat/ifi/INF5072/v18/2018_jan25_quic_sferlin.pdf)有詳盡的介紹\n\n- [Persistent \"pipes\" in Linux](https://gist.github.com/CAFxX/571a1558db9a7b393579)\n  - persistent pipe ([pipe](https://man7.org/linux/man-pages/man2/pipe.2.html) 為 Linux 重新導向資料的方法) \n  - hole punching 為 Linux [fallocate](https要://man7.org/linux/man-pages/man2/fallocate.2.html) 提供的特性，可以標注一個檔案哪段不要 (the logical size of the file does not change; only the physical size (the number of blocks) may change)\n  - 利用這個特性把檔案當作 queue 來使用且 lseek 可以跳過 hole\n\n- [Why kafka is so fast](https://medium.com/swlh/why-kafka-is-so-fast-bde0d987cd03)\n  - 實時並不代表快，而是可預測\n  - 針對吞吐量去最佳化\n  - log-structured -\u003e append-only log -\u003e sequential 很多系統也都採用，有興趣可參考[此書](https://www.databass.dev)\n  - record batch 像 [redis pipeline](https://redis.io/topics/pipelining) 一樣，遇到網路問題的話還是要為了減少 RTT 還做 batch\n  - cheap consumers 不移除 message 而是增加 offset，並依靠之前的 sequential read\n  - unflushed buffered writes 不會 fsync -\u003e \\\n  disk-backed in-memory queue -\u003e\\\n  一個 ack 的 write 並不代表 durability -\u003e\\\n  用 replica 來 cover\n  - \n- RCU\n  - [Linux 核心設計: RCU 同步機制](https://hackmd.io/@sysprog/linux-rcu)\n  - [What is RCU, Fundamentally?](http://lwn.net/Articles/262464/)\n  - [What is RCU? Part 2: Usage](https://lwn.net/Articles/263130/)\n  - [RCU part 3: the RCU API](https://lwn.net/Articles/264090/)\n  - [Linux中的RCU的那點事](https://www.huaweicloud.com/articles/74854d8cc300d73a76b7b526044f6709.html)\n  -  one writer multiple readers. mainly require 3 mechanisms to implement it.\n        1. Publish-Subscribe Mechanism (insertion)\n        1. Wait For Pre-Existing RCU Readers to Complete (deletion)\n        1. Maintain Multiple Versions of Recently Updated Objects (reader update)\n  - RCU 並不保證一定能讀到新增的節點或不讀到要被刪除的節點 (grace period)\n  - RCU 讓 reader 不需要 lock 和不需要擔心 [memory barrier](https://en.wikipedia.org/wiki/Memory_barrier) (為了 instruction pipeline 效能提昇而讓 instruction 亂序執行， memory barrier 是為了避免亂序而使用) 且有不錯的效能"},"nextPost":{"slug":"openssl-escape-character","frontmatter":{"title":"Openssl escape character","date":"January 8, 2022","description":"Be careful about the escape character as Linux input"},"excerpt":"","content":" \nFirst of all, I pick [HTTP signature](https://datatracker.ietf.org/doc/html/draft-cavage-http-signatures-12) as my authorization method.\n\nI use [Vegeta](https://github.com/tsenart/vegeta) to load test my API service.\n\nWhen writing a script for target generation, I need to make a signature for each request.\n\nSo I used `openssl` to generate it.\n```\necho -n \"${message}\" | openssl dgst -sha256 -hmac \"${secret}\" -binary | base64\n```\nBut I did not pass my authenticator.\n\nAfter some trying(use other language to reproduce what happened), I found that message used for generation was weird.(check the byte)\n\nGo:\n```go\nimport (\n\t\"crypto/hmac\"\n\t\"crypto/sha256\"\n\tb64 \"encoding/base64\"\n)\n// https://go.dev/play/p/JaDyZh_wNqn\nfunc hmacSha256(data string, secret string) string {\n\th := hmac.New(sha256.New, []byte(secret))\n\th.Write([]byte(data))\n\treturn b64.URLEncoding.EncodeToString(h.Sum(nil))\n}\n```\n\nNode:\n```javascript\nvar crypto = require('crypto');\nfunction hmacSha256(message, secret) {\n    hmac = crypto.createHmac('sha256', secret);\n    hmac.update(message);\n    return hmac.digest('base64')\n}\n```\n\nWhen make `a\\nmessage` as argument in Linux, it would be the same as `a\\\\nmessage`.\n\nUse \n```\necho -e -n \"${message}\" | openssl dgst -sha256 -hmac \"${secret}\" -binary | base64\n``` \ninstead.\n\nThis was why my target generation script didn't pass the authenticator.\n\nI had definitely known how the escape character works, but I didn't check it in the first place.\n\nWhen using escape character as Linux input, you should be careful about it.\n\nOpenssl might be a misplace but never mind. I found the problem from it.\n\nJust write a note to remember the time wasted on it."}},"__N_SSG":true},"page":"/post/[slug]","query":{"slug":"cache-policy"},"buildId":"Z0_U_cDDsSEVLkTYwbyDH","assetPrefix":"/blog","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>